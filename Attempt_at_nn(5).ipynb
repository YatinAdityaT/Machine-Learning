{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "iris =  pd.read_csv(\"/home/yatin/Downloads/IRIS.csv\")\n",
    "iris = iris.sample(frac=1).reset_index(drop=True) # Shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = iris[[\"sepal_length\",\"sepal_width\",\"petal_length\",\"petal_width\"]]\n",
    "X = np.array(X)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "one_hot_encoder = OneHotEncoder(sparse=False)\n",
    "\n",
    "Y = iris.species\n",
    "# print(Y)\n",
    "Y = one_hot_encoder.fit_transform(np.array(Y).reshape(-1, 1))\n",
    "Y[0:5]\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape (4, 120) \n",
      "Y_train.shape (3, 120) \n",
      "X_test (4, 30) \n",
      "Y_test (3, 30)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20)\n",
    "X_train=X_train.T\n",
    "Y_train=Y_train.T\n",
    "X_test=X_test.T\n",
    "Y_test=Y_test.T\n",
    "print(\"X_train.shape\",X_train.shape,\"\\nY_train.shape\",Y_train.shape,\"\\nX_test\",X_test.shape,\"\\nY_test\",Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers_dims, print_shapes = False):\n",
    "    \n",
    "    L = len(layers_dims)\n",
    "    parameters ={}\n",
    "    \n",
    "    for i in range(1,L) :\n",
    "        parameters['W'+ str(i)] = np.random.rand(layers_dims[i],layers_dims[i-1])*0.01\n",
    "        parameters['b'+ str(i)] = np.zeros((layers_dims[i],1))\n",
    "        \n",
    "    if print_shapes :\n",
    "        initialize_parameters_print_shapes(parameters)\n",
    "#     if print_all:\n",
    "#         initialize_parameters_print_all(parameters)\n",
    "\n",
    "    return parameters        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_print_shapes(parameters):\n",
    "    print('initialised:\\n')\n",
    "    for i in range(1,L):\n",
    "        print('\\nW'+ str(i),parameters['W'+ str(i)].shape)\n",
    "        print('\\nb'+ str(i),parameters['b'+ str(i)].shape)\n",
    "    print('--------------------------------------------')\n",
    "\n",
    "# def initialize_parameters_print_all(parameters):\n",
    "#     print('initialised:\\n')\n",
    "#     for i in range(1,L):\n",
    "# #         print(parameters['W1'])\n",
    "#         print('W'+ str(i),parameters['W'+ str(i)])\n",
    "#         print('b'+ str(i),parameters['b'+ str(i)])\n",
    "#     print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large S(y_i) = \\frac {e^{y_i}}{\\sum_{j=1}^{i} y_j}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def relu(Z):\n",
    "#     A = np.maximum(Z,0,Z)\n",
    "#     return A\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    return A\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "#     print(e_x)\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "# def softmax(x):\n",
    "#     return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "# scores = [3.0, 1.60, 0.0]\n",
    "# print(softmax(scores))\n",
    "# print(softmax2(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation(X,parameters,layers_dims,print_para = False):\n",
    "    \n",
    "    L = len(layers_dims)\n",
    "    A_prev = X\n",
    "    \n",
    "    for i in range(1,L-1): # 1st hidden layer to last hidden layer\n",
    "        parameters ['Z'+ str(i)] = np.dot(parameters ['W'+ str(i)],A_prev) + parameters ['b'+ str(i)]\n",
    "        parameters ['A'+ str(i)] = sigmoid(parameters ['Z'+ str(i)])\n",
    "        A_prev = parameters ['A'+ str(i)]\n",
    "    # for the output layer we will use softmax\n",
    "    parameters ['Z'+ str(i+1)] = np.dot(parameters ['W'+ str(i+1)],A_prev) + parameters ['b'+ str(i+1)]\n",
    "    parameters ['AL'] = softmax(parameters ['Z'+ str(i+1)])\n",
    "#     parameters ['AL'] = parameters.pop('A4') \n",
    "    \n",
    "    if print_para:\n",
    "        forward_propogation_print_shapes()\n",
    "            \n",
    "    return parameters ['AL'] ,parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propogation_print_shapes():\n",
    "    print('F_propogated:\\n')\n",
    "    for i in range(1,L):\n",
    "        print('\\n','Z'+str(i),parameters['Z'+str(i)].shape)\n",
    "        print('\\n','W'+str(i),parameters['W'+str(i)].shape)\n",
    "        print('\\n','b'+str(i),parameters['b'+str(i)].shape)\n",
    "    print('\\n','AL',parameters['AL'].shape)\n",
    "    print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialised:\n",
      "\n",
      "\n",
      "W1 (5, 3)\n",
      "\n",
      "b1 (5, 1)\n",
      "\n",
      "W2 (4, 5)\n",
      "\n",
      "b2 (4, 1)\n",
      "\n",
      "W3 (2, 4)\n",
      "\n",
      "b3 (2, 1)\n",
      "\n",
      "W4 (1, 2)\n",
      "\n",
      "b4 (1, 1)\n",
      "--------------------------------------------\n",
      "F_propogated:\n",
      "\n",
      "\n",
      " Z1 (5, 40)\n",
      "\n",
      " W1 (5, 3)\n",
      "\n",
      " b1 (5, 1)\n",
      "\n",
      " Z2 (4, 40)\n",
      "\n",
      " W2 (4, 5)\n",
      "\n",
      " b2 (4, 1)\n",
      "\n",
      " Z3 (2, 40)\n",
      "\n",
      " W3 (2, 4)\n",
      "\n",
      " b3 (2, 1)\n",
      "\n",
      " Z4 (1, 40)\n",
      "\n",
      " W4 (1, 2)\n",
      "\n",
      " b4 (1, 1)\n",
      "\n",
      " AL (1, 40)\n",
      "--------------------------------------------\n",
      "{'W1': array([[0.00034878, 0.00510372, 0.00703926],\n",
      "       [0.00778153, 0.00914418, 0.00604234],\n",
      "       [0.00631437, 0.00867913, 0.00333925],\n",
      "       [0.00776167, 0.00644953, 0.00234977],\n",
      "       [0.00721444, 0.00894101, 0.00889146]]), 'b1': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W2': array([[0.00803508, 0.00433229, 0.0005106 , 0.00437116, 0.00961894],\n",
      "       [0.00762857, 0.00580332, 0.00552627, 0.00012769, 0.0077222 ],\n",
      "       [0.00127722, 0.00527965, 0.00167553, 0.00027758, 0.00311945],\n",
      "       [0.00350425, 0.00257669, 0.00380403, 0.00600102, 0.00796784]]), 'b2': array([[0.],\n",
      "       [0.],\n",
      "       [0.],\n",
      "       [0.]]), 'W3': array([[0.00533394, 0.00526639, 0.00456331, 0.00344393],\n",
      "       [0.00406989, 0.00517495, 0.00769782, 0.00855203]]), 'b3': array([[0.],\n",
      "       [0.]]), 'W4': array([[0.00482181, 0.00118278]]), 'b4': array([[0.]]), 'Z1': array([[0.00790619, 0.00613743, 0.00356934, 0.00827568, 0.00465078,\n",
      "        0.00548269, 0.00381115, 0.0077417 , 0.00671474, 0.00823362,\n",
      "        0.0070496 , 0.00450647, 0.01055852, 0.00919293, 0.00353264,\n",
      "        0.00600441, 0.00856187, 0.00694897, 0.0077829 , 0.00716414,\n",
      "        0.0033869 , 0.00696804, 0.00712277, 0.00299648, 0.00777683,\n",
      "        0.00509685, 0.00712079, 0.0045631 , 0.00790926, 0.00891538,\n",
      "        0.01012539, 0.00220042, 0.01013151, 0.00816465, 0.00691133,\n",
      "        0.00540523, 0.00649699, 0.00748745, 0.00572502, 0.00456297],\n",
      "       [0.01130555, 0.00976089, 0.01185793, 0.01412834, 0.01126551,\n",
      "        0.00765583, 0.01033725, 0.00791004, 0.00986052, 0.01193739,\n",
      "        0.01390332, 0.01018937, 0.01979926, 0.01691679, 0.01195445,\n",
      "        0.00914769, 0.01552146, 0.01161661, 0.01146729, 0.01283256,\n",
      "        0.00860724, 0.01343596, 0.01094083, 0.01117396, 0.01567699,\n",
      "        0.01501246, 0.01163614, 0.0108977 , 0.01632119, 0.0107272 ,\n",
      "        0.0183902 , 0.00852453, 0.01948546, 0.01092561, 0.00705835,\n",
      "        0.00865637, 0.0132332 , 0.01610801, 0.00968547, 0.00635288],\n",
      "       [0.00905791, 0.00723562, 0.01019649, 0.01021477, 0.00935879,\n",
      "        0.00602844, 0.00882368, 0.00511409, 0.00723338, 0.00838945,\n",
      "        0.01022679, 0.00853961, 0.01600551, 0.01337717, 0.00982271,\n",
      "        0.00692472, 0.01172585, 0.00982905, 0.00960095, 0.01108262,\n",
      "        0.00649965, 0.0097091 , 0.00829895, 0.00905516, 0.01321471,\n",
      "        0.01326509, 0.00817572, 0.00918894, 0.01324774, 0.0081372 ,\n",
      "        0.01467514, 0.00737515, 0.01571905, 0.00819227, 0.00420416,\n",
      "        0.00620995, 0.00984109, 0.01284691, 0.0076261 , 0.00495051],\n",
      "       [0.00717428, 0.00660663, 0.01011125, 0.00991361, 0.00892361,\n",
      "        0.00479217, 0.00842952, 0.00385575, 0.00639224, 0.00769954,\n",
      "        0.01035425, 0.00790076, 0.01438216, 0.0122085 , 0.01025369,\n",
      "        0.00604392, 0.01115926, 0.00799693, 0.00738678, 0.00910676,\n",
      "        0.00694403, 0.00992891, 0.00726034, 0.0097604 , 0.01168364,\n",
      "        0.01246178, 0.00800785, 0.00858962, 0.01229404, 0.00604603,\n",
      "        0.01319181, 0.00746948, 0.0143001 , 0.00667119, 0.0034574 ,\n",
      "        0.00589067, 0.00996782, 0.01232208, 0.00673514, 0.00397184],\n",
      "       [0.01284155, 0.01132942, 0.01148018, 0.01641264, 0.01154773,\n",
      "        0.00881722, 0.0102731 , 0.01061171, 0.01166197, 0.0144354 ,\n",
      "        0.01568002, 0.01047861, 0.02137021, 0.01854298, 0.01190779,\n",
      "        0.0105837 , 0.01747761, 0.01242962, 0.01263201, 0.01339921,\n",
      "        0.00927666, 0.01533554, 0.01262728, 0.01113455, 0.01632176,\n",
      "        0.01443528, 0.01379504, 0.01108512, 0.01733068, 0.01297306,\n",
      "        0.0200954 , 0.0081059 , 0.02097729, 0.01300617, 0.00974882,\n",
      "        0.01019635, 0.01477581, 0.01718331, 0.01078363, 0.007361  ]]), 'A1': array([[0.50197654, 0.50153435, 0.50089233, 0.50206891, 0.50116269,\n",
      "        0.50137067, 0.50095279, 0.50193542, 0.50167868, 0.50205839,\n",
      "        0.50176239, 0.50112662, 0.50263961, 0.50229822, 0.50088316,\n",
      "        0.5015011 , 0.50214045, 0.50173723, 0.50194572, 0.50179103,\n",
      "        0.50084673, 0.501742  , 0.50178069, 0.50074912, 0.5019442 ,\n",
      "        0.50127421, 0.50178019, 0.50114077, 0.5019773 , 0.50222883,\n",
      "        0.50253133, 0.5005501 , 0.50253286, 0.50204115, 0.50172783,\n",
      "        0.50135131, 0.50162424, 0.50187185, 0.50143125, 0.50114074],\n",
      "       [0.50282636, 0.5024402 , 0.50296445, 0.50353203, 0.50281635,\n",
      "        0.50191395, 0.50258429, 0.5019775 , 0.50246511, 0.50298431,\n",
      "        0.50347577, 0.50254732, 0.50494965, 0.5042291 , 0.50298858,\n",
      "        0.50228691, 0.50388029, 0.50290412, 0.50286679, 0.50320809,\n",
      "        0.5021518 , 0.50335894, 0.50273518, 0.50279346, 0.50391917,\n",
      "        0.50375304, 0.502909  , 0.5027244 , 0.50408021, 0.50268177,\n",
      "        0.50459742, 0.50213112, 0.50487121, 0.50273138, 0.50176458,\n",
      "        0.50216408, 0.50330825, 0.50402691, 0.50242135, 0.50158821],\n",
      "       [0.50226446, 0.5018089 , 0.5025491 , 0.50255367, 0.50233968,\n",
      "        0.5015071 , 0.50220591, 0.50127852, 0.50180834, 0.50209735,\n",
      "        0.50255668, 0.50213489, 0.50400129, 0.50334424, 0.50245566,\n",
      "        0.50173117, 0.50293143, 0.50245724, 0.50240022, 0.50277063,\n",
      "        0.50162491, 0.50242726, 0.50207473, 0.50226377, 0.50330363,\n",
      "        0.50331622, 0.50204392, 0.50229722, 0.50331189, 0.50203429,\n",
      "        0.50366872, 0.50184378, 0.50392968, 0.50204806, 0.50105104,\n",
      "        0.50155248, 0.50246025, 0.50321168, 0.50190652, 0.50123762],\n",
      "       [0.50179356, 0.50165165, 0.50252779, 0.50247838, 0.50223089,\n",
      "        0.50119804, 0.50210737, 0.50096394, 0.50159806, 0.50192488,\n",
      "        0.50258854, 0.50197518, 0.50359548, 0.50305209, 0.5025634 ,\n",
      "        0.50151098, 0.50278979, 0.50199922, 0.50184669, 0.50227667,\n",
      "        0.501736  , 0.50248221, 0.50181508, 0.50244008, 0.50292088,\n",
      "        0.5031154 , 0.50200195, 0.50214739, 0.50307347, 0.5015115 ,\n",
      "        0.5032979 , 0.50186736, 0.50357496, 0.50166779, 0.50086435,\n",
      "        0.50147266, 0.50249193, 0.50308048, 0.50168378, 0.50099296],\n",
      "       [0.50321034, 0.50283232, 0.50287001, 0.50410307, 0.5028869 ,\n",
      "        0.50220429, 0.50256825, 0.5026529 , 0.50291546, 0.50360879,\n",
      "        0.50391992, 0.50261963, 0.50534235, 0.50463561, 0.50297691,\n",
      "        0.5026459 , 0.50436929, 0.50310737, 0.50315796, 0.50334975,\n",
      "        0.50231915, 0.50383381, 0.50315678, 0.50278361, 0.50408035,\n",
      "        0.50360876, 0.5034487 , 0.50277125, 0.50433256, 0.50324322,\n",
      "        0.50502368, 0.50202647, 0.50524413, 0.5032515 , 0.50243719,\n",
      "        0.50254906, 0.50369389, 0.50429572, 0.50269588, 0.50184024]]), 'Z2': array([[0.01350204, 0.01349233, 0.01349401, 0.01351757, 0.0134943 ,\n",
      "        0.01348056, 0.01348793, 0.01348854, 0.01349416, 0.01350771,\n",
      "        0.01351359, 0.01348905, 0.01354584, 0.01353047, 0.01349518,\n",
      "        0.01348895, 0.01352377, 0.01350046, 0.01350177, 0.01350592,\n",
      "        0.01348089, 0.01351156, 0.01349956, 0.01349076, 0.01352034,\n",
      "        0.01351056, 0.01350392, 0.01349223, 0.01352441, 0.01350241,\n",
      "        0.01353891, 0.01347629, 0.01354357, 0.01350189, 0.01348333,\n",
      "        0.01348602, 0.0135091 , 0.01352295, 0.0134903 , 0.01347276],\n",
      "       [0.01347304, 0.01346197, 0.01346461, 0.01348642, 0.01346475,\n",
      "        0.01345109, 0.01345858, 0.01345794, 0.01346385, 0.01347675,\n",
      "        0.01348237, 0.01345968, 0.01351671, 0.01350077, 0.01346499,\n",
      "        0.01345894, 0.01349317, 0.01347196, 0.01347339, 0.01347778,\n",
      "        0.01345008, 0.01348014, 0.01346956, 0.01346027, 0.01349174,\n",
      "        0.01348212, 0.01347267, 0.01346291, 0.01349494, 0.01347307,\n",
      "        0.01350951, 0.01344667, 0.01351429, 0.01347209, 0.01345219,\n",
      "        0.01345535, 0.01347805, 0.01349299, 0.01346057, 0.01344312],\n",
      "       [0.00584646, 0.00584188, 0.00584543, 0.00585377, 0.00584461,\n",
      "        0.0058363 , 0.00584187, 0.00583831, 0.00584244, 0.0058484 ,\n",
      "        0.00585254, 0.0058419 , 0.00586858, 0.00586089, 0.00584573,\n",
      "        0.00584028, 0.00585725, 0.00584663, 0.00584672, 0.00584966,\n",
      "        0.00583759, 0.00585139, 0.00584525, 0.00584357, 0.00585696,\n",
      "        0.00585383, 0.00584708, 0.00584364, 0.0058587 , 0.00584566,\n",
      "        0.00586495, 0.00583659, 0.0058676 , 0.00584578, 0.00583584,\n",
      "        0.00583882, 0.00585059, 0.005858  , 0.00584139, 0.00583264],\n",
      "       [0.01198608, 0.01197794, 0.01198541, 0.01200054, 0.01198353,\n",
      "        0.01196713, 0.01197841, 0.01197058, 0.01197885, 0.0119901 ,\n",
      "        0.01199854, 0.01197827, 0.01202828, 0.01201384, 0.01198615,\n",
      "        0.0119748 , 0.01200712, 0.01198659, 0.01198649, 0.01199235,\n",
      "        0.0119705 , 0.01199635, 0.01198414, 0.01198217, 0.01200643,\n",
      "        0.01200111, 0.01198791, 0.01198164, 0.01200992, 0.01198428,\n",
      "        0.0120214 , 0.0119687 , 0.01202653, 0.01198481, 0.01196612,\n",
      "        0.01197228, 0.01199487, 0.01200878, 0.011977  , 0.01196033]]), 'A2': array([[0.50337546, 0.50337303, 0.50337345, 0.50337934, 0.50337352,\n",
      "        0.50337009, 0.50337193, 0.50337208, 0.50337349, 0.50337688,\n",
      "        0.50337835, 0.50337221, 0.50338641, 0.50338257, 0.50337374,\n",
      "        0.50337219, 0.50338089, 0.50337506, 0.50337539, 0.50337643,\n",
      "        0.50337017, 0.50337784, 0.50337484, 0.50337264, 0.50338003,\n",
      "        0.50337759, 0.50337593, 0.50337301, 0.50338105, 0.50337555,\n",
      "        0.50338468, 0.50336902, 0.50338584, 0.50337542, 0.50337078,\n",
      "        0.50337145, 0.50337722, 0.50338069, 0.50337252, 0.50336814],\n",
      "       [0.50336821, 0.50336544, 0.5033661 , 0.50337155, 0.50336614,\n",
      "        0.50336272, 0.5033646 , 0.50336443, 0.50336591, 0.50336914,\n",
      "        0.50337054, 0.50336487, 0.50337913, 0.50337514, 0.5033662 ,\n",
      "        0.50336468, 0.50337324, 0.50336794, 0.5033683 , 0.50336939,\n",
      "        0.50336247, 0.50336998, 0.50336734, 0.50336502, 0.50337288,\n",
      "        0.50337048, 0.50336812, 0.50336568, 0.50337368, 0.50336822,\n",
      "        0.50337733, 0.50336162, 0.50337852, 0.50336797, 0.503363  ,\n",
      "        0.50336379, 0.50336946, 0.5033732 , 0.50336509, 0.50336073],\n",
      "       [0.50146161, 0.50146047, 0.50146135, 0.50146344, 0.50146115,\n",
      "        0.50145907, 0.50146046, 0.50145957, 0.50146061, 0.5014621 ,\n",
      "        0.50146313, 0.50146047, 0.50146714, 0.50146522, 0.50146143,\n",
      "        0.50146007, 0.50146431, 0.50146165, 0.50146168, 0.50146241,\n",
      "        0.50145939, 0.50146284, 0.50146131, 0.50146089, 0.50146424,\n",
      "        0.50146345, 0.50146177, 0.50146091, 0.50146467, 0.50146141,\n",
      "        0.50146623, 0.50145914, 0.5014669 , 0.50146144, 0.50145896,\n",
      "        0.5014597 , 0.50146264, 0.5014645 , 0.50146034, 0.50145816],\n",
      "       [0.50299648, 0.50299445, 0.50299632, 0.5030001 , 0.50299585,\n",
      "        0.50299175, 0.50299457, 0.50299261, 0.50299468, 0.50299749,\n",
      "        0.5029996 , 0.50299453, 0.50300703, 0.50300342, 0.5029965 ,\n",
      "        0.50299366, 0.50300174, 0.50299661, 0.50299659, 0.50299805,\n",
      "        0.50299259, 0.50299905, 0.502996  , 0.50299551, 0.50300157,\n",
      "        0.50300024, 0.50299694, 0.50299537, 0.50300244, 0.50299604,\n",
      "        0.50300531, 0.50299214, 0.5030066 , 0.50299617, 0.50299149,\n",
      "        0.50299303, 0.50299868, 0.50300216, 0.50299422, 0.50299005]]), 'Z3': array([[0.00935652, 0.00935648, 0.0093565 , 0.00935658, 0.0093565 ,\n",
      "        0.00935644, 0.00935647, 0.00935646, 0.00935649, 0.00935654,\n",
      "        0.00935657, 0.00935647, 0.0093567 , 0.00935664, 0.0093565 ,\n",
      "        0.00935647, 0.00935661, 0.00935652, 0.00935652, 0.00935654,\n",
      "        0.00935644, 0.00935656, 0.00935651, 0.00935648, 0.0093566 ,\n",
      "        0.00935657, 0.00935652, 0.00935649, 0.00935661, 0.00935652,\n",
      "        0.00935667, 0.00935643, 0.00935669, 0.00935652, 0.00935644,\n",
      "        0.00935646, 0.00935655, 0.00935661, 0.00935647, 0.0093564 ],\n",
      "       [0.01281539, 0.01281534, 0.01281536, 0.01281547, 0.01281536,\n",
      "        0.01281528, 0.01281533, 0.01281531, 0.01281534, 0.01281541,\n",
      "        0.01281545, 0.01281533, 0.01281562, 0.01281554, 0.01281537,\n",
      "        0.01281532, 0.0128155 , 0.01281539, 0.01281539, 0.01281542,\n",
      "        0.01281529, 0.01281544, 0.01281537, 0.01281535, 0.01281549,\n",
      "        0.01281545, 0.01281539, 0.01281535, 0.01281551, 0.01281538,\n",
      "        0.01281558, 0.01281527, 0.01281561, 0.01281538, 0.01281528,\n",
      "        0.0128153 , 0.01281543, 0.01281551, 0.01281533, 0.01281524]]), 'A3': array([[0.50233911, 0.5023391 , 0.50233911, 0.50233913, 0.50233911,\n",
      "        0.50233909, 0.5023391 , 0.5023391 , 0.5023391 , 0.50233912,\n",
      "        0.50233912, 0.5023391 , 0.50233916, 0.50233914, 0.50233911,\n",
      "        0.5023391 , 0.50233913, 0.50233911, 0.50233911, 0.50233912,\n",
      "        0.50233909, 0.50233912, 0.50233911, 0.5023391 , 0.50233913,\n",
      "        0.50233912, 0.50233911, 0.5023391 , 0.50233914, 0.50233911,\n",
      "        0.50233915, 0.50233909, 0.50233916, 0.50233911, 0.50233909,\n",
      "        0.5023391 , 0.50233912, 0.50233913, 0.5023391 , 0.50233908],\n",
      "       [0.5032038 , 0.50320379, 0.5032038 , 0.50320382, 0.5032038 ,\n",
      "        0.50320378, 0.50320379, 0.50320378, 0.50320379, 0.50320381,\n",
      "        0.50320382, 0.50320379, 0.50320386, 0.50320384, 0.5032038 ,\n",
      "        0.50320379, 0.50320383, 0.5032038 , 0.5032038 , 0.50320381,\n",
      "        0.50320378, 0.50320382, 0.5032038 , 0.50320379, 0.50320383,\n",
      "        0.50320382, 0.5032038 , 0.50320379, 0.50320383, 0.5032038 ,\n",
      "        0.50320385, 0.50320377, 0.50320386, 0.5032038 , 0.50320378,\n",
      "        0.50320378, 0.50320381, 0.50320383, 0.50320379, 0.50320377]]), 'Z4': array([[0.00301736, 0.00301736, 0.00301736, 0.00301736, 0.00301736,\n",
      "        0.00301736, 0.00301736, 0.00301736, 0.00301736, 0.00301736,\n",
      "        0.00301736, 0.00301736, 0.00301736, 0.00301736, 0.00301736,\n",
      "        0.00301736, 0.00301736, 0.00301736, 0.00301736, 0.00301736,\n",
      "        0.00301736, 0.00301736, 0.00301736, 0.00301736, 0.00301736,\n",
      "        0.00301736, 0.00301736, 0.00301736, 0.00301736, 0.00301736,\n",
      "        0.00301736, 0.00301736, 0.00301736, 0.00301736, 0.00301736,\n",
      "        0.00301736, 0.00301736, 0.00301736, 0.00301736, 0.00301736]]), 'AL': array([[0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
      "        0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
      "        0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
      "        0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025, 0.025,\n",
      "        0.025, 0.025, 0.025, 0.025]])}\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [3,5,4,2,1]\n",
    "L = len(layers_dims)\n",
    "parameters = initialize_parameters(layers_dims, print_shapes = True )\n",
    "X = np.random.rand(3,40)\n",
    "AL,parameters = forward_propogation(X,parameters,layers_dims,L)\n",
    "print(parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\large Cost = -\\frac{1}{m}\\sum_{i=1}^m(y\\times log(h(x))+(1-y)\\times log(1-h(x)))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Y,AL):\n",
    "    e=1e-5\n",
    "    m = Y.shape[1]\n",
    "    cost = (-1/m)*np.sum(np.multiply(Y,np.log(AL+e),np.multiply(1-Y,np.log(1-AL+e))))\n",
    "    return(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Y = np.random.rand(2,2)\n",
    "# AL = np.random.rand(2,2)\n",
    "# cost=compute_cost(Y,AL)\n",
    "# print(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\delta Z = (\\frac{-Y}{AL} - \\frac{1-Y}{1-AL})\\times\\sigma\\prime(Z)  =  \\delta A\\times\\sigma\\prime(Z) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_backwards(cache):\n",
    "    return softmax(cache)*(1-softmax(cache))\n",
    "\n",
    "def sigmoid_backwards(cache):\n",
    "    return sigmoid(cache)*(1-sigmoid(cache))\n",
    "\n",
    "def tanh_backwards(Z):\n",
    "    return 1 - np.tanh(Z)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "$\\large\\delta A^{[l]} = (\\frac{-Y}{AL} - \\frac{1-Y}{1-AL})$\n",
    "\n",
    "$\\large\\delta Z^{[l]} =  \\delta A\\times\\sigma\\prime(Z) $\n",
    "\n",
    "$\\large\\delta W^{[l]} =  \\frac {1}{m}\\delta Z . A^{[l-1]T}$\n",
    "\n",
    "$\\large\\delta b^{[l]} = \\frac {1}{m}\\sum _{i=1}^{m}\\delta Z$\n",
    "\n",
    "$\\large\\delta A^{[l-1]} = W^{[l]T} . dZ^{[l]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropogation(layers_dims,parameters,AL,Y,X,print_shapes = False):\n",
    "    caches = {}\n",
    "    m = Y.shape[1]\n",
    "    L = len(layers_dims)\n",
    "    flag = True\n",
    "    parameters['A' + str(0)] = X\n",
    "    for l in reversed(range(1,L)):\n",
    "        \n",
    "        if (flag) :   \n",
    "            caches['dAL'] = np.divide(-Y,AL)-np.divide(1-Y,1-AL)                               \n",
    "            caches['dZ'+ str(l)] = caches['dAL']*sigmoid_backwards(parameters['Z'+ str(l)])    \n",
    "            caches['dW'+ str(l)] = (1/m)*np.dot(caches['dZ'+ str(l)],parameters['A' + str(l-1)].T)          \n",
    "            caches['db'+ str(l)] = (1/m)*np.sum(caches['dZ'+ str(l)],axis=1,keepdims=True)     \n",
    "            caches['dA' + str(l-1)] = np.dot(parameters['W'+ str(l)].T,caches['dZ' + str(l)])\n",
    "            flag = False\n",
    "        else:\n",
    "            caches['dZ'+ str(l)] = caches['dA' + str(l)]*softmax_backwards(parameters['Z'+ str(l)])\n",
    "            caches['dW'+ str(l)] = (1/m)*np.dot(caches['dZ'+ str(l)],parameters['A' + str(l-1)].T)\n",
    "            caches['db'+ str(l)] = (1/m)*np.sum(caches['dZ'+ str(l)],axis=1,keepdims=True)\n",
    "            if (l-1 != 0) :\n",
    "                caches['dA' + str(l-1)] = np.dot(parameters['W' + str(l)].T,caches['dZ' + str(l)]) \n",
    "                \n",
    "    if print_shapes :\n",
    "        backpropogation_print_shapes(caches)\n",
    "    \n",
    "    return caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropogation_print_shapes(caches):\n",
    "    print('B_propogated:\\n')\n",
    "    print ('dAL',caches['dAL'].shape)\n",
    "    for l in range(1,L):\n",
    "        print(\"dZ\"+ str(l),caches['dZ'+ str(l)].shape)\n",
    "        print(\"dW\"+ str(l),caches['dW'+ str(l)].shape)\n",
    "        print(\"db\"+ str(l),caches['db'+ str(l)].shape)\n",
    "        if (l-1 != 0) :\n",
    "            print('dA' + str(l-1),caches['dA' + str(l-1)].shape)\n",
    "    print('--------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [3,5,4,2,1]\n",
    "L = len(layers_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialised:\n",
      "\n",
      "\n",
      "W1 (5, 3)\n",
      "\n",
      "b1 (5, 1)\n",
      "\n",
      "W2 (4, 5)\n",
      "\n",
      "b2 (4, 1)\n",
      "\n",
      "W3 (2, 4)\n",
      "\n",
      "b3 (2, 1)\n",
      "\n",
      "W4 (2, 2)\n",
      "\n",
      "b4 (2, 1)\n",
      "--------------------------------------------\n",
      "F_propogated:\n",
      "\n",
      "\n",
      " Z1 (5, 140)\n",
      "\n",
      " W1 (5, 3)\n",
      "\n",
      " b1 (5, 1)\n",
      "\n",
      " Z2 (4, 140)\n",
      "\n",
      " W2 (4, 5)\n",
      "\n",
      " b2 (4, 1)\n",
      "\n",
      " Z3 (2, 140)\n",
      "\n",
      " W3 (2, 4)\n",
      "\n",
      " b3 (2, 1)\n",
      "\n",
      " Z4 (2, 140)\n",
      "\n",
      " W4 (2, 2)\n",
      "\n",
      " b4 (2, 1)\n",
      "\n",
      " AL (2, 140)\n",
      "--------------------------------------------\n",
      "B_propogated:\n",
      "\n",
      "dAL (2, 140)\n",
      "dZ1 (5, 140)\n",
      "dW1 (5, 3)\n",
      "db1 (5, 1)\n",
      "dZ2 (4, 140)\n",
      "dW2 (4, 5)\n",
      "db2 (4, 1)\n",
      "dA1 (5, 140)\n",
      "dZ3 (2, 140)\n",
      "dW3 (2, 4)\n",
      "db3 (2, 1)\n",
      "dA2 (4, 140)\n",
      "dZ4 (2, 140)\n",
      "dW4 (2, 2)\n",
      "db4 (2, 1)\n",
      "dA3 (2, 140)\n",
      "--------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [3,5,4,2,2]\n",
    "L = len(layers_dims)\n",
    "parameters = initialize_parameters(layers_dims,print_shapes=True)\n",
    "X = np.random.rand(3,140)\n",
    "Y = np.random.rand(1,140)\n",
    "AL,cache = forward_propogation(X,parameters,layers_dims,print_para = True)\n",
    "# forward_propogation_print_shapes()\n",
    "caches = backpropogation(layers_dims,cache,AL,Y,X,print_shapes=True)\n",
    "# print(caches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters,caches,alpha,L):\n",
    "    for l in range(L-1):\n",
    "        parameters['W'+ str(l+1)] = parameters['W'+ str(l+1)] - alpha * caches['dW'+ str(l+1)]\n",
    "        parameters['b'+ str(l+1)] = parameters['b'+ str(l+1)] - alpha * caches['db'+ str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_updated = update_parameters(cache,caches,0.01,L)\n",
    "# print(cache_updated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters,layers_dims):\n",
    "    m = X.shape[1]\n",
    "    n = len(layers_dims)  \n",
    "#     p = np.zeros(Y.shape)\n",
    "    AL, caches = forward_propogation(X, parameters,layers_dims)\n",
    "    \n",
    "#     _max = AL[0][0]  \n",
    "#     for i in range(0, AL.shape[1]):\n",
    "#         for j in range(0,AL.shape[0]):\n",
    "#             if (AL[i][j] > _max):\n",
    "#                 p, _max = j,AL[i][j]\n",
    "#         for k in range(0,AL.shape[0]):   \n",
    "#             if (k == p) :\n",
    "#                 AL[i][k] = 1\n",
    "#             else :\n",
    "#                 AL[i][k] = 0\n",
    "    print(AL)\n",
    "    for i in range(AL.shape[1]):\n",
    "        _max = AL[:,i].argmax()\n",
    "        AL[_max,i] = 1\n",
    "        for j in range(AL.shape[0]):\n",
    "            if AL[j,i] != 1 :\n",
    "                AL[j,i] = 0\n",
    "            \n",
    "    print(\"Accuracy: \"  + str(np.sum((AL == Y)/m)*100))\n",
    "        \n",
    "    return AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn( X, Y, layers_dims, iterations,alpha, print_cost= False):\n",
    "    parameters = initialize_parameters(layers_dims, print_shapes = False)\n",
    "    costs = []\n",
    "    for i in range(1,iterations):\n",
    "        \n",
    "        AL ,caches = forward_propogation(X,parameters,layers_dims)\n",
    "        cost = compute_cost(Y,AL)\n",
    "        caches = backpropogation(layers_dims,parameters,AL,Y,X,print_shapes = False)\n",
    "        parameters = update_parameters(parameters,caches,alpha,len(layers_dims))\n",
    "        \n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        costs.append(cost)\n",
    "                  \n",
    "    p = predict(X, Y, parameters,layers_dims)\n",
    "    return p,costs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing the nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 100: 5.881883\n",
      "Cost after iteration 200: 5.881885\n",
      "Cost after iteration 300: 5.881886\n",
      "Cost after iteration 400: 5.881886\n",
      "Cost after iteration 500: 5.881887\n",
      "Cost after iteration 600: 5.881887\n",
      "Cost after iteration 700: 5.881887\n",
      "Cost after iteration 800: 5.881887\n",
      "Cost after iteration 900: 5.881887\n",
      "Cost after iteration 1000: 5.881887\n",
      "Cost after iteration 1100: 5.881887\n",
      "Cost after iteration 1200: 5.881887\n",
      "Cost after iteration 1300: 5.881887\n",
      "Cost after iteration 1400: 5.881887\n",
      "Cost after iteration 1500: 5.881887\n",
      "Cost after iteration 1600: 5.881887\n",
      "Cost after iteration 1700: 5.881887\n",
      "Cost after iteration 1800: 5.881887\n",
      "Cost after iteration 1900: 5.881887\n",
      "Cost after iteration 2000: 5.881887\n",
      "Cost after iteration 2100: 5.881887\n",
      "Cost after iteration 2200: 5.881887\n",
      "Cost after iteration 2300: 5.881887\n",
      "Cost after iteration 2400: 5.881887\n",
      "Cost after iteration 2500: 5.881887\n",
      "Cost after iteration 2600: 5.881887\n",
      "Cost after iteration 2700: 5.881887\n",
      "Cost after iteration 2800: 5.881887\n",
      "Cost after iteration 2900: 5.881887\n",
      "Cost after iteration 3000: 5.881887\n",
      "Cost after iteration 3100: 5.881887\n",
      "Cost after iteration 3200: 5.881887\n",
      "Cost after iteration 3300: 5.881887\n",
      "Cost after iteration 3400: 5.881887\n",
      "Cost after iteration 3500: 5.881887\n",
      "Cost after iteration 3600: 5.881887\n",
      "Cost after iteration 3700: 5.881887\n",
      "Cost after iteration 3800: 5.881887\n",
      "Cost after iteration 3900: 5.881887\n",
      "Cost after iteration 4000: 5.881887\n",
      "Cost after iteration 4100: 5.881887\n",
      "Cost after iteration 4200: 5.881887\n",
      "Cost after iteration 4300: 5.881887\n",
      "Cost after iteration 4400: 5.881887\n",
      "Cost after iteration 4500: 5.881887\n",
      "Cost after iteration 4600: 5.881887\n",
      "Cost after iteration 4700: 5.881887\n",
      "Cost after iteration 4800: 5.881887\n",
      "Cost after iteration 4900: 5.881887\n",
      "Cost after iteration 5000: 5.881887\n",
      "Cost after iteration 5100: 5.881887\n",
      "Cost after iteration 5200: 5.881887\n",
      "Cost after iteration 5300: 5.881887\n",
      "Cost after iteration 5400: 5.881887\n",
      "Cost after iteration 5500: 5.881887\n",
      "Cost after iteration 5600: 5.881887\n",
      "Cost after iteration 5700: 5.881887\n",
      "Cost after iteration 5800: 5.881887\n",
      "Cost after iteration 5900: 5.881887\n",
      "Cost after iteration 6000: 5.881887\n",
      "Cost after iteration 6100: 5.881887\n",
      "Cost after iteration 6200: 5.881887\n",
      "Cost after iteration 6300: 5.881887\n",
      "Cost after iteration 6400: 5.881887\n",
      "[[0.00277846 0.00277847 0.00277841 0.00277847 0.00277849 0.00277849\n",
      "  0.00277851 0.00277848 0.00277841 0.00277847 0.0027784  0.00277843\n",
      "  0.0027785  0.00277842 0.00277841 0.00277844 0.00277847 0.00277841\n",
      "  0.0027784  0.00277841 0.00277845 0.0027784  0.00277846 0.00277845\n",
      "  0.00277845 0.00277841 0.00277841 0.0027784  0.0027784  0.00277841\n",
      "  0.00277841 0.00277847 0.00277845 0.0027784  0.00277848 0.00277846\n",
      "  0.0027784  0.0027784  0.00277845 0.00277848 0.00277843 0.0027784\n",
      "  0.00277847 0.00277842 0.00277844 0.00277841 0.00277842 0.00277846\n",
      "  0.00277841 0.00277842 0.00277846 0.00277841 0.00277845 0.00277846\n",
      "  0.00277848 0.00277841 0.00277846 0.00277841 0.0027784  0.00277847\n",
      "  0.0027784  0.0027785  0.00277848 0.00277846 0.00277849 0.00277846\n",
      "  0.00277841 0.0027784  0.00277849 0.00277842 0.00277848 0.00277847\n",
      "  0.00277847 0.00277846 0.00277843 0.00277844 0.00277845 0.00277845\n",
      "  0.00277849 0.00277844 0.00277846 0.00277841 0.00277848 0.00277848\n",
      "  0.00277846 0.00277843 0.00277841 0.00277849 0.00277839 0.00277844\n",
      "  0.00277844 0.00277844 0.00277846 0.00277845 0.00277846 0.00277841\n",
      "  0.00277847 0.00277844 0.00277846 0.00277849 0.00277848 0.00277845\n",
      "  0.00277848 0.00277847 0.00277841 0.00277849 0.00277848 0.0027785\n",
      "  0.00277849 0.00277844 0.00277841 0.00277845 0.00277846 0.00277849\n",
      "  0.00277841 0.00277846 0.0027784  0.00277846 0.00277847 0.00277851]\n",
      " [0.00284688 0.00284689 0.00284683 0.00284689 0.0028469  0.0028469\n",
      "  0.00284693 0.0028469  0.00284683 0.00284688 0.00284682 0.00284684\n",
      "  0.00284692 0.00284683 0.00284683 0.00284686 0.00284688 0.00284682\n",
      "  0.00284682 0.00284683 0.00284687 0.00284682 0.00284688 0.00284687\n",
      "  0.00284687 0.00284683 0.00284682 0.00284682 0.00284681 0.00284683\n",
      "  0.00284683 0.00284689 0.00284686 0.00284682 0.0028469  0.00284688\n",
      "  0.00284682 0.00284682 0.00284687 0.0028469  0.00284684 0.00284682\n",
      "  0.00284689 0.00284683 0.00284686 0.00284683 0.00284684 0.00284688\n",
      "  0.00284683 0.00284683 0.00284688 0.00284683 0.00284686 0.00284688\n",
      "  0.0028469  0.00284682 0.00284688 0.00284683 0.00284682 0.00284689\n",
      "  0.00284682 0.00284692 0.0028469  0.00284688 0.00284691 0.00284688\n",
      "  0.00284683 0.00284682 0.00284691 0.00284684 0.0028469  0.00284688\n",
      "  0.00284688 0.00284688 0.00284684 0.00284686 0.00284687 0.00284686\n",
      "  0.00284691 0.00284685 0.00284688 0.00284683 0.0028469  0.0028469\n",
      "  0.00284688 0.00284684 0.00284682 0.00284691 0.00284681 0.00284686\n",
      "  0.00284686 0.00284686 0.00284688 0.00284687 0.00284688 0.00284683\n",
      "  0.00284689 0.00284686 0.00284688 0.00284691 0.00284689 0.00284687\n",
      "  0.0028469  0.00284689 0.00284682 0.00284691 0.0028469  0.00284692\n",
      "  0.00284691 0.00284686 0.00284683 0.00284687 0.00284688 0.00284691\n",
      "  0.00284683 0.00284688 0.00284682 0.00284687 0.00284688 0.00284693]\n",
      " [0.00270803 0.00270805 0.00270799 0.00270805 0.00270806 0.00270806\n",
      "  0.00270808 0.00270806 0.00270799 0.00270804 0.00270798 0.002708\n",
      "  0.00270807 0.00270799 0.00270799 0.00270802 0.00270804 0.00270798\n",
      "  0.00270798 0.00270799 0.00270803 0.00270798 0.00270804 0.00270803\n",
      "  0.00270802 0.00270799 0.00270798 0.00270798 0.00270798 0.00270799\n",
      "  0.00270799 0.00270804 0.00270802 0.00270798 0.00270805 0.00270804\n",
      "  0.00270798 0.00270798 0.00270803 0.00270805 0.002708   0.00270798\n",
      "  0.00270805 0.00270799 0.00270802 0.00270799 0.002708   0.00270803\n",
      "  0.00270799 0.00270799 0.00270804 0.00270799 0.00270802 0.00270804\n",
      "  0.00270805 0.00270798 0.00270803 0.00270799 0.00270798 0.00270804\n",
      "  0.00270798 0.00270807 0.00270805 0.00270804 0.00270806 0.00270804\n",
      "  0.00270799 0.00270798 0.00270806 0.002708   0.00270805 0.00270804\n",
      "  0.00270804 0.00270804 0.002708   0.00270801 0.00270803 0.00270802\n",
      "  0.00270807 0.00270801 0.00270804 0.00270799 0.00270806 0.00270806\n",
      "  0.00270804 0.002708   0.00270798 0.00270806 0.00270797 0.00270802\n",
      "  0.00270802 0.00270801 0.00270803 0.00270803 0.00270804 0.00270799\n",
      "  0.00270804 0.00270802 0.00270804 0.00270806 0.00270805 0.00270803\n",
      "  0.00270806 0.00270804 0.00270798 0.00270806 0.00270806 0.00270808\n",
      "  0.00270806 0.00270802 0.00270799 0.00270803 0.00270804 0.00270807\n",
      "  0.00270799 0.00270804 0.00270798 0.00270803 0.00270804 0.00270808]]\n",
      "Accuracy: 170.00000000000003\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f75f251b0f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "itera,alpha = 6500,0.01\n",
    "layers_dims = [X_train.shape[0],5,4,2,Y_train.shape[0]]\n",
    "p,costs = nn( X_train, Y_train, layers_dims,itera,alpha, print_cost= True)\n",
    "# print(p)\n",
    "\n",
    "import seaborn as sns\n",
    "# print(costs)\n",
    "sns.lineplot(x= np.arange(1,itera),y =costs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
